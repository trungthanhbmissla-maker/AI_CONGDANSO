import os
from flask import Flask, jsonify, request
from flask_cors import CORS
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# ====================================================
# 1Ô∏è‚É£ C·∫•u h√¨nh ban ƒë·∫ßu
# ====================================================
load_dotenv()

app = Flask(__name__)
CORS(app)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY or GOOGLE_API_KEY.strip() == "":
    GOOGLE_API_KEY = "AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxx" #ƒê∆∞a l√™n onrender c·∫ßn thay ƒë·ªïi ƒë·ªÉ kh√¥ng l·ªô key
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY trong .env ‚Üí ƒëang d√πng key d·ª± ph√≤ng trong code.")

if not GOOGLE_API_KEY:
    raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY. Vui l√≤ng ƒë·∫∑t trong file .env ho·∫∑c trong code fallback.")

try:
    genai.configure(api_key=GOOGLE_API_KEY)
    print("‚úÖ C·∫•u h√¨nh Gemini API th√†nh c√¥ng.")
except Exception as e:
    print(f"‚ùå L·ªói khi c·∫•u h√¨nh Gemini API: {e}")
    raise

MODELS_TO_TRY = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-2.0-flash-lite",
    "gemini-2.0-flash-lite-001",
    "gemini-2.5-pro-preview-tts",
    "gemini-2.5-flash-lite",
    "gemini-2.5-flash-lite-preview-09-2025",
    "gemini-2.5-computer-use-preview-10-2025"
    "gemini-3-flash-preview",
    "gemini-3-pro-preview"
]

# ====================================================
# 2Ô∏è‚É£ H√†m g·ªçi Gemini an to√†n
# ====================================================
'''def generate_text(prompt, safety_settings=None, generation_config=None):
    """
    G·ªçi Gemini an to√†n v·ªõi nhi·ªÅu fallback:
    - h·ªó tr·ª£ response.candidates[*].content.parts (c≈©)
    - h·ªó tr·ª£ response.text ho·∫∑c response.output_text (m·ªõi)
    - lu√¥n tr·∫£ v·ªÅ string (kh√¥ng tr·∫£ None) ‚Äî n·∫øu kh√¥ng c√≥ n·ªôi dung s·∫Ω tr·∫£ message th√¥ng b√°o
    """
    safety_settings = safety_settings or []
    generation_config = generation_config or {"max_output_tokens": 300, "temperature": 0.8}

    last_error = None
    for model_name in MODELS_TO_TRY:
        try:
            print(f"üîÑ Th·ª≠ model: {model_name}")
            model = genai.GenerativeModel(model_name=model_name)
            response = model.generate_content(
                prompt,
                safety_settings=safety_settings,
                generation_config=generation_config,
            )

            # 1) N·∫øu ƒë·ªëi t∆∞·ª£ng response c√≥ thu·ªôc t√≠nh 'candidates' ki·ªÉu c≈©
            try:
                if hasattr(response, "candidates") and response.candidates:
                    candidate = response.candidates[0]
                    # candidate.content.parts (tr∆∞·ªùng h·ª£p b·∫°n d√πng tr∆∞·ªõc)
                    if hasattr(candidate, "content") and candidate.content:
                        parts = getattr(candidate.content, "parts", None)
                        if parts:
                            text = "".join([getattr(p, "text", "") for p in parts if getattr(p, "text", None)])
                            if text and text.strip():
                                print(f"‚úÖ Th√†nh c√¥ng (candidates.parts) v·ªõi {model_name}")
                                return text.strip()

                    # fallback: candidate may have text directly
                    if hasattr(candidate, "text") and candidate.text:
                        t = candidate.text.strip()
                        if t:
                            print(f"‚úÖ Th√†nh c√¥ng (candidate.text) v·ªõi {model_name}")
                            return t

            except Exception as e:
                # kh√¥ng d·ª´ng, th·ª≠ c√°c d·∫°ng kh√°c
                print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ candidates: {e}")

            # 2) N·∫øu response c√≥ .text ho·∫∑c .output_text (m·ªôt s·ªë SDK tr·∫£ text tr·ª±c ti·∫øp)
            if hasattr(response, "text") and response.text:
                t = response.text.strip()
                if t:
                    print(f"‚úÖ Th√†nh c√¥ng (response.text) v·ªõi {model_name}")
                    return t

            if hasattr(response, "output_text") and response.output_text:
                t = response.output_text.strip()
                if t:
                    print(f"‚úÖ Th√†nh c√¥ng (response.output_text) v·ªõi {model_name}")
                    return t

            # 3) M·ªôt s·ªë API tr·∫£ dict-like trong str form; c·ªë parse fallback
            try:
                # Convert to string and return non-empty
                s = str(response)
                if s and len(s) > 20:  # tr√°nh tr·∫£ c√°c chu·ªói ng·∫Øn v√¥ nghƒ©a
                    print(f"‚úÖ Th√†nh c√¥ng (str(response)) v·ªõi {model_name}")
                    return s.strip()
            except Exception:
                pass

            # n·∫øu ƒë·∫øn ƒë√¢y: response kh√¥ng ch·ª©a text r√µ r√†ng, ti·∫øp t·ª•c model kh√°c
            last_error = f"No text in response for model {model_name}"
            print(f"‚ö†Ô∏è {last_error}")

        except ResourceExhausted:
            print(f"‚ö†Ô∏è Model {model_name} h·∫øt quota, th·ª≠ model kh√°c...")
            last_error = f"ResourceExhausted:{model_name}"
            continue
        except Exception as e:
            print(f"‚ùå L·ªói g·ªçi model {model_name}: {e}")
            last_error = str(e)
            continue

    # N·∫øu kh√¥ng c√≥ model n√†o tr·∫£ v·ªÅ n·ªôi dung h·ª£p l·ªá -> tr·∫£ fallback r√µ r√†ng
    fallback_msg = ("‚ö†Ô∏è H·ªá th·ªëng AI hi·ªán kh√¥ng tr·∫£ n·ªôi dung r√µ r√†ng. "
                    "Xin th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá qu·∫£n tr·ªã vi√™n. "
                    "Chi ti·∫øt l·ªói: " + (last_error or "unknown"))
    print(fallback_msg)
    return fallback_msg'''
def generate_text(prompt, safety_settings=None, generation_config=None):
    #raise RuntimeError("üî• ƒêANG V√ÄO generate_text M·ªöI üî•")
    safety_settings = safety_settings or []
    generation_config = generation_config or {
        "max_output_tokens": 300,
        "temperature": 0.8
    }

    last_error = None

    for model_name in MODELS_TO_TRY:
        print(f"üîÑ ƒêANG TH·ª¨ MODEL: {model_name}")

        try:
            model = genai.GenerativeModel(model_name=model_name)
            response = model.generate_content(
                prompt,
                safety_settings=safety_settings,
                generation_config=generation_config,
            )

            # ===== L·∫§Y TEXT =====
            if hasattr(response, "text") and response.text:
                print(f"‚úÖ OK: response.text t·ª´ {model_name}")
                return response.text.strip()

            if hasattr(response, "candidates") and response.candidates:
                c = response.candidates[0]
                if hasattr(c, "content") and hasattr(c.content, "parts"):
                    text = "".join(p.text for p in c.content.parts if hasattr(p, "text"))
                    if text.strip():
                        print(f"‚úÖ OK: candidates.parts t·ª´ {model_name}")
                        return text.strip()

            raise ValueError("Kh√¥ng c√≥ text h·ª£p l·ªá trong response")

        except ResourceExhausted as e:
            print(f"‚ö†Ô∏è QUOTA H·∫æT t·∫°i {model_name}: {e}")
            last_error = f"ResourceExhausted:{model_name}"
            continue

        except Exception as e:
            print(f"‚ùå MODEL {model_name} L·ªñI: {type(e).__name__} | {e}")
            last_error = f"{model_name}:{e}"
            continue

    return (
        "‚ö†Ô∏è H·ªá th·ªëng AI hi·ªán kh√¥ng kh·∫£ d·ª•ng.\n"
        "Vui l√≤ng th·ª≠ l·∫°i sau.\n"
        f"Chi ti·∫øt l·ªói cu·ªëi: {last_error}"
    )
# ====================================================
# üõ∞Ô∏è TR·∫†M 1 ‚Äì KHAI TH√ÅC D·ªÆ LI·ªÜU & TH√îNG TIN
# ====================================================
@app.route("/api/station1-info-literacy", methods=["POST"])
def station1_info_literacy():
    try:
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "generate_task")
        answer = data.get("answer", "")
        task = data.get("task", "")
        topic = data.get("topic", "tin gi·∫£ v·ªÅ tr∆∞·ªùng h·ªçc")

        if mode == "generate_task":
            prompt = f"""
            B·∫°n l√† AI gi√°o d·ª•c CHIRON26 gi√∫p h·ªçc sinh r√®n luy·ªán k·ªπ nƒÉng x√°c th·ª±c th√¥ng tin.
            H√£y t·∫°o **2 c√¢u h·ªèi tr·∫Øc nghi·ªám A/B** xoay quanh ch·ªß ƒë·ªÅ "{topic}" (tin gi·∫£, th√¥ng tin sai l·ªách).
            
            C·∫•u tr√∫c b·∫Øt bu·ªôc:
            T√åNH HU·ªêNG 1: [M√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ m·ªôt tin ƒë·ªìn/tin gi·∫£]
            C√ÇU H·ªéI: [C√¢u h·ªèi v·ªÅ h√†nh ƒë·ªông n√™n l√†m]
            A. [H√†nh ƒë·ªông sai/thi·∫øu ki·ªÉm ch·ª©ng]
            B. [H√†nh ƒë·ªông ƒë√∫ng/ki·ªÉm ch·ª©ng ngu·ªìn tin]
            
            (T∆∞∆°ng t·ª± cho T√¨nh hu·ªëng 2)
            
            QUAN TR·ªåNG: KH√îNG ƒë∆∞·ª£c k√®m theo ƒë√°p √°n ƒë√∫ng hay l·ªùi gi·∫£i th√≠ch ·ªü cu·ªëi (ƒë·ªÉ h·ªçc sinh t·ª± l√†m).
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.8})
            return jsonify({"station": 1, "response": result})

        elif mode == "evaluate":
            prompt = f"""
            B·∫°n l√† gi√°m kh·∫£o ch·∫•m thi tr·∫Øc nghi·ªám k·ªπ nƒÉng th√¥ng tin.
            
            1. ƒê·ªÄ B√ÄI:
            {task}

            2. H·ªåC SINH CH·ªåN:
            {answer}

            3. Y√äU C·∫¶U CH·∫§M:
            - X√°c ƒë·ªãnh ƒë√°p √°n ƒë√∫ng c·ªßa t·ª´ng t√¨nh hu·ªëng (ƒê√°p √°n ƒë√∫ng l√† h√†nh ƒë·ªông ki·ªÉm ch·ª©ng th√¥ng tin, t√¨m ngu·ªìn ch√≠nh th·ªëng).
            - So s√°nh v·ªõi l·ª±a ch·ªçn c·ªßa h·ªçc sinh.
            - T√≠nh t·ªïng s·ªë c√¢u ƒë√∫ng (tr√™n 2 c√¢u).

            4. ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI B·∫ÆT BU·ªòC (D√≤ng ƒë·∫ßu ti√™n):
            SCORE: x/2
            
            (Trong ƒë√≥ x l√† s·ªë c√¢u ƒë√∫ng. V√≠ d·ª•: SCORE: 0/2, SCORE: 1/2, SCORE: 2/2).
            
            Sau d√≤ng SCORE m·ªõi ƒë∆∞·ª£c vi·∫øt gi·∫£i th√≠ch chi ti·∫øt v√¨ sao ƒë√∫ng/sai.
            """
            
            # Gi·∫£m temperature xu·ªëng 0.5 ƒë·ªÉ ch·∫•m ƒëi·ªÉm ch√≠nh x√°c, kh√¥ng "vƒÉn v·ªü"
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.5})
            return jsonify({"station": 1, "feedback": result})

        else:
            return jsonify({"error": "Invalid mode"}), 400

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 1:", e)
        return jsonify({"error": str(e)}), 500


# ====================================================
# üí¨ TR·∫†M 2 ‚Äì GIAO TI·∫æP & H·ª¢P T√ÅC S·ªê
# ====================================================
@app.route("/api/station2-digital-collab", methods=["POST"])
def station2_digital_collab():
    try:
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "generate_task")
        answer = data.get("answer", "")
        task = data.get("task", "")

        if mode == "generate_task":
            prompt = """
            B·∫°n l√† AI gi√°o d·ª•c CHIRON26 gi√∫p h·ªçc sinh hu·∫•n luy·ªán k·ªπ nƒÉng giao ti·∫øp & h·ª£p t√°c trong m√¥i tr∆∞·ªùng s·ªë.
            H√£y t·∫°o **2 t√¨nh hu·ªëng ng·∫Øn (2‚Äì3 c√¢u)** v·ªÅ h·ªçc sinh l√†m vi·ªác nh√≥m tr·ª±c tuy·∫øn,
            m·ªói t√¨nh hu·ªëng c√≥ **m·ªôt c√¢u h·ªèi tr·∫Øc nghi·ªám A/B** ƒë·ªÉ h·ªçc sinh ch·ªçn c√°ch ·ª©ng x·ª≠ ƒë√∫ng.
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.85})
            return jsonify({"station": 2, "response": result})

        elif mode == "evaluate":
            prompt = f"""
            Nhi·ªám v·ª•:
            {task}

            C√¢u tr·∫£ l·ªùi c·ªßa h·ªçc sinh:
            {answer}

            H√£y ch·∫•m v√† ph·∫£n h·ªìi ng·∫Øn g·ªçn, n√™u ƒëi·ªÉm m·∫°nh/y·∫øu trong h·ª£p t√°c s·ªë.
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.75})
            return jsonify({"station": 2, "feedback": result})

        else:
            return jsonify({"error": "Invalid mode"}), 400

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 2:", e)
        return jsonify({"error": str(e)}), 500


# ====================================================
# üé® TR·∫†M 3 ‚Äì S√ÅNG T·∫†O N·ªòI DUNG S·ªê (chat)
# ====================================================

@app.route("/api/station3-content-creation", methods=["POST"])
def station3_content_creation():
    try:
        data = request.get_json(silent=True) or {}
        user_message = data.get("message", "")
        if not user_message:
            user_message = "Xin ch√†o, t√¥i mu·ªën t·∫°o n·ªôi dung s·ªë an to√†n v√† s√°ng t·∫°o."

        prompt = f"""
        B·∫°n l√† AI gi√°o d·ª•c CHIRON26 gi√∫p h·ªçc sinh hu·∫•n luy·ªán nƒÉng l·ª±c s√°ng t·∫°o n·ªôi dung s·ªë.
        H·ªçc sinh n√≥i: "{user_message}"
        H√£y ph·∫£n h·ªìi nh∆∞ c·ªë v·∫•n s√°ng t·∫°o, g·ª£i √Ω √Ω t∆∞·ªüng (b√†i ƒëƒÉng, video, poster)
        v√† nh·∫•n m·∫°nh ƒë·∫°o ƒë·ª©c, b·∫£n quy·ªÅn, tr√°ch nhi·ªám khi s√°ng t·∫°o n·ªôi dung.
        Kh√¥ng n√™n qu√° d√†i, ch·ªâ c·∫ßn kho·∫£ng 10 d√≤ng.
        """
        text = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.85})
        return jsonify({"station": 3, "response": text})

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 3:", e)
        return jsonify({"error": str(e)}), 500


# ====================================================
# üõ°Ô∏è TR·∫†M 4 ‚Äì AN TO√ÄN S·ªê
# ====================================================
@app.route("/api/station4-safety", methods=["POST"])
def station4_safety():
    try:
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "generate_task")
        answer = data.get("answer", "")
        task = data.get("task", "")

        if mode == "generate_task":
            # Th√™m y√™u c·∫ßu KH√îNG k√®m ƒë√°p √°n ƒë·ªÉ tr√°nh l·ªô
            prompt = """
            B·∫°n l√† AI gi√°o d·ª•c CHIRON26 gi√∫p h·ªçc sinh hu·∫•n luy·ªán k·ªπ nƒÉng an to√†n s·ªë.
            H√£y t·∫°o **2 t√¨nh hu·ªëng tr·∫Øc nghi·ªám ng·∫Øn** v·ªÅ: b·∫£o m·∫≠t t√†i kho·∫£n, l·ª´a ƒë·∫£o tr·ª±c tuy·∫øn (phishing), ho·∫∑c b·∫Øt n·∫°t m·∫°ng.
            
            C·∫•u tr√∫c b·∫Øt bu·ªôc:
            T√åNH HU·ªêNG 1: [N·ªôi dung]
            C√ÇU H·ªéI: [C√¢u h·ªèi]
            A. [L·ª±a ch·ªçn 1]
            B. [L·ª±a ch·ªçn 2]
            
            (T∆∞∆°ng t·ª± cho T√¨nh hu·ªëng 2)
            QUAN TR·ªåNG: KH√îNG ƒë∆∞·ª£c vi·∫øt ƒë√°p √°n ƒë√∫ng hay gi·∫£i th√≠ch ·ªü cu·ªëi.
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.8})
            return jsonify({"station": 4, "response": result})

        elif mode == "evaluate":
            # --- S·ª¨A QUAN TR·ªåNG T·∫†I ƒê√ÇY ---
            prompt = f"""
            B·∫°n l√† gi√°m kh·∫£o ch·∫•m thi tr·∫Øc nghi·ªám v·ªÅ AN TO√ÄN S·ªê.
            
            1. ƒê·ªÄ B√ÄI:
            {task}

            2. H·ªåC SINH CH·ªåN:
            {answer}

            3. Y√äU C·∫¶U:
            - X√°c ƒë·ªãnh ƒë√°p √°n ƒë√∫ng d·ª±a tr√™n ki·∫øn th·ª©c an to√†n th√¥ng tin.
            - So s√°nh v·ªõi ƒë√°p √°n h·ªçc sinh ch·ªçn.
            - ƒê·∫øm s·ªë c√¢u ƒë√∫ng (tr√™n t·ªïng s·ªë 2 c√¢u).

            4. ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI B·∫ÆT BU·ªòC (D√≤ng ƒë·∫ßu ti√™n):
            SCORE: x/2
            
            (Trong ƒë√≥ x l√† s·ªë c√¢u ƒë√∫ng. V√≠ d·ª•: SCORE: 0/2, SCORE: 1/2, SCORE: 2/2).
            
            Sau d√≤ng SCORE m·ªõi ƒë∆∞·ª£c vi·∫øt ph·∫ßn gi·∫£i th√≠ch chi ti·∫øt ƒë√∫ng/sai cho t·ª´ng c√¢u.
            KH√îNG ch√∫c m·ª´ng n·∫øu h·ªçc sinh l√†m sai.
            """
            
            # Gi·∫£m temperature xu·ªëng 0.5 ƒë·ªÉ AI ch·∫•m nghi√™m t√∫c h∆°n, √≠t "s√°ng t·∫°o" lung tung
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.5})
            return jsonify({"station": 4, "feedback": result})

        else:
            return jsonify({"error": "Invalid mode"}), 400

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 4:", e)
        return jsonify({"error": str(e)}), 500

# ====================================================
# üß© TR·∫†M 5 ‚Äì GI·∫¢I QUY·∫æT V·∫§N ƒê·ªÄ
# ====================================================
@app.route("/api/station5-problem-solving", methods=["POST"])
def station5_problem_solving():
    try:
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "generate_task")
        answer = data.get("answer", "")
        task = data.get("task", "")

        if mode == "generate_task":
            prompt = """
            B·∫°n l√† AI gi√°o d·ª•c CHIRON26 gi√∫p h·ªçc sinh hu·∫•n luy·ªán k·ªπ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ b·∫±ng c√¥ng ngh·ªá s·ªë.
            H√£y t·∫°o **2 t√¨nh hu·ªëng (3‚Äì4 c√¢u)** m√¥ t·∫£ s·ª± c·ªë k·ªπ thu·∫≠t (m·∫•t d·ªØ li·ªáu, l·ªói ph·∫ßn m·ªÅm...),
            m·ªói t√¨nh hu·ªëng c√≥ m·ªôt c√¢u h·ªèi tr·∫Øc nghi·ªám A/B g·ª£i √Ω c√°ch x·ª≠ l√Ω.
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.8})
            return jsonify({"station": 5, "response": result})

        elif mode == "evaluate":
            prompt = f"""
            Nhi·ªám v·ª•:
            {task}

            C√¢u tr·∫£ l·ªùi c·ªßa h·ªçc sinh:
            {answer}

            H√£y ch·∫•m v√† ph·∫£n h·ªìi logic v·ªõi vai tr√≤ l√† AI gi√°o d·ª•c h·ªó tr·ª£ cho h·ªçc sinh, khuy·∫øn kh√≠ch h·ªçc sinh √°p d·ª•ng quy tr√¨nh: 
            x√°c ƒë·ªãnh nguy√™n nh√¢n ‚Äì th·ª≠ gi·∫£i ph√°p ‚Äì ƒë√°nh gi√° k·∫øt qu·∫£.
            """
            result = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.75})
            return jsonify({"station": 5, "feedback": result})

        else:
            return jsonify({"error": "Invalid mode"}), 400

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 5:", e)
        return jsonify({"error": str(e)}), 500



# ====================================================
# ü§ñ TR·∫†M 6 ‚Äì ƒê·∫†O ƒê·ª®C & TR√ç TU·ªÜ NH√ÇN T·∫†O (chat)
# ====================================================
@app.route("/api/station6-ai-ethics", methods=["POST"])
def station6_ai_ethics():
    try:
        data = request.get_json(silent=True) or {}
        user_message = data.get("message", "")
        if not user_message:
            user_message = "Ch√∫ng ta n√™n d√πng AI nh∆∞ th·∫ø n√†o ƒë·ªÉ c√≥ tr√°ch nhi·ªám?"
        prompt = f"""
        B·∫°n l√† AI gi√°o d·ª•c c√≥ t√™n CHIRON26. B·∫°n s·∫Ω c√πng th·∫£o lu·∫≠n v·ªõi h·ªçc sinh v·ªÅ **·ª®ng d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o**.
        H·ªçc sinh n√≥i: "{user_message}"
        H√£y ph·∫£n h·ªìi b·∫±ng c√°ch g·ª£i m·ªü, gi√∫p h·ªçc sinh hi·ªÉu:
        - AI n√™n ƒë∆∞·ª£c d√πng c√≥ tr√°ch nhi·ªám, c√¥ng b·∫±ng, an to√†n.
        - Tr√°nh l·∫°m d·ª•ng, sao ch√©p ho·∫∑c t·∫°o n·ªôi dung g√¢y h·∫°i.
        - Kh√¥ng qu√° d√†i, kh√¥ng qu√° 10 d√≤ng.
        """
        text = generate_text(prompt, generation_config={"max_output_tokens": 2048, "temperature": 0.7})
        return jsonify({"station": 6, "response": text})

    except Exception as e:
        print("‚ùå L·ªói tr·∫°m 6:", e)
        return jsonify({"error": str(e)}), 500


# ====================================================
# ‚úÖ KI·ªÇM TRA ROUTE
# ====================================================
@app.route("/", methods=["GET"])
def home():
    return jsonify({
        "routes": [
            "/api/station1-info-literacy",
            "/api/station2-digital-collab",
            "/api/station3-content-creation",
            "/api/station4-safety",
            "/api/station5-problem-solving",
            "/api/station6-ai-ethics"
        ],
        "status": "‚úÖ Backend AI C√¥ng d√¢n s·ªë ƒëang ch·∫°y"
    })


# ====================================================
# üîü Ch·∫°y server
# ====================================================
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    app.run(host="0.0.0.0", port=port, debug=False)
